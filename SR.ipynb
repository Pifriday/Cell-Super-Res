{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell Super Resolution\n",
    "\n",
    "The purpose of this notebook is to test how well out-of-focus microscope images can be brought into focus through machine learning. The images used in this notebook were downloaded from the [Broad Institute website](https://data.broadinstitute.org/bbbc/BBBC006/). The dataset contains 1536 unique images of human U2OS cells with half the z-stack being unfocused and half being optimally focused."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from os import listdir\n",
    "\n",
    "from tqdm import tqdm\n",
    "from IPython.display import HTML\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "from skimage import exposure\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first step we create an array containing the paths for the unfocused and focused images in each pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['./Data/BBBC006_v1_images_z_00/mcf-z-stacks-03212011_b01_s1_w2a96697ba-9495-4e9c-9b6f-dfb7b2e6fa5b.tif',\n",
       "        './Data/BBBC006_v1_images_z_16/mcf-z-stacks-03212011_b01_s1_w2fef744d4-6ad1-4908-869c-3efa2fdc4f6f.tif'],\n",
       "       ['./Data/BBBC006_v1_images_z_00/mcf-z-stacks-03212011_p10_s1_w245153e95-f2fb-4b8c-83e7-d8a78181472d.tif',\n",
       "        './Data/BBBC006_v1_images_z_16/mcf-z-stacks-03212011_p10_s1_w29e2c619b-1208-4b86-bcf4-13db612245e8.tif'],\n",
       "       ['./Data/BBBC006_v1_images_z_00/mcf-z-stacks-03212011_f09_s1_w2c8bedd9b-25ea-498d-b211-d1063e24a12b.tif',\n",
       "        './Data/BBBC006_v1_images_z_16/mcf-z-stacks-03212011_f09_s1_w229562219-8057-419a-86c7-5eadd5d1aec9.tif'],\n",
       "       ...,\n",
       "       ['./Data/BBBC006_v1_images_z_00/mcf-z-stacks-03212011_m16_s1_w2fc2fdf8a-ba4a-45e6-9faf-938785561b0d.tif',\n",
       "        './Data/BBBC006_v1_images_z_16/mcf-z-stacks-03212011_m16_s1_w2dca0fe55-7dcd-4203-bb6b-f1112ec38a90.tif'],\n",
       "       ['./Data/BBBC006_v1_images_z_00/mcf-z-stacks-03212011_e04_s2_w18d71492e-8c1d-4dc6-8adf-b347aa99b483.tif',\n",
       "        './Data/BBBC006_v1_images_z_16/mcf-z-stacks-03212011_e04_s2_w102b81c38-9c64-4685-ab9a-734aabddca18.tif'],\n",
       "       ['./Data/BBBC006_v1_images_z_00/mcf-z-stacks-03212011_o14_s1_w1af10bb48-4885-44e5-bec0-957f0f50d7d9.tif',\n",
       "        './Data/BBBC006_v1_images_z_16/mcf-z-stacks-03212011_o14_s1_w1ef99f6a0-f409-473d-b10c-021f35f0a84f.tif']],\n",
       "      dtype='<U101')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths = []\n",
    "image_pairs = []\n",
    "\n",
    "data_dir = './Data/'\n",
    "focused_dir = 'BBBC006_v1_images_z_16' # Folder containing the focused images\n",
    "\n",
    "# The file names for matching images contain these characters in common\n",
    "match_start = 51\n",
    "match_end = 61\n",
    "\n",
    "for folder in listdir(data_dir):\n",
    "    for file in listdir(data_dir + folder):\n",
    "        file_path = f'{data_dir + folder}/{file}'\n",
    "        \n",
    "        # If the other half of the pair is in the list, add them both to the image_pairs list\n",
    "        match_list = list(filter(lambda x: x[match_start:match_end] == file_path[match_start:match_end], paths))\n",
    "        \n",
    "        if len(match_list):\n",
    "            match = match_list[0]\n",
    "            row = {}\n",
    "            \n",
    "            if focused_dir in match:\n",
    "                row['unfocused_path'] = file_path\n",
    "                row['focused_path'] = match\n",
    "            else:\n",
    "                row['unfocused_path'] = match\n",
    "                row['focused_path'] = file_path    \n",
    "\n",
    "            image_pairs.append([row['unfocused_path'], row['focused_path']])\n",
    "            \n",
    "        else:\n",
    "            paths.append(file_path)\n",
    "                  \n",
    "image_pairs = np.array(image_pairs)\n",
    "image_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1536, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_pairs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all the pairs, we can collect the data that will be used to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1524/1524 [17:28<00:00,  1.45it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(551566080, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set, test_set = train_test_split(image_pairs, test_size = 1/128 , random_state = 13)\n",
    "\n",
    "total_pixel_count = len(train_set)*696*520\n",
    "pixel_data = np.empty((total_pixel_count, 4))\n",
    "solution_data = np.empty(total_pixel_count)\n",
    "\n",
    "count = 0\n",
    "\n",
    "for pair in tqdm(train_set):\n",
    "    unfocused_file = plt.imread(pair[0])\n",
    "    focused_file = plt.imread(pair[1])\n",
    "    \n",
    "    flat = unfocused_file.flatten()\n",
    "    mean = flat.mean() // 1\n",
    "    std = flat.std() // 1\n",
    "    ptp = np.ptp(flat)\n",
    "\n",
    "    # For every pixel in every unfocused image, save the color\n",
    "    for x, col in enumerate(unfocused_file):\n",
    "        for y, current_pixel in enumerate(col):\n",
    "\n",
    "            pixel_data[count] = (current_pixel, mean, std, ptp)\n",
    "\n",
    "            # Also save the corresponding pixel for the focused image\n",
    "            solution_data[count] = focused_file[x][y]\n",
    "            count += 1\n",
    "\n",
    "pixel_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[143., 136.,   6.,  57.],\n",
       "       [127., 136.,   6.,  57.],\n",
       "       [136., 136.,   6.,  57.],\n",
       "       ...,\n",
       "       [416., 428.,  56., 924.],\n",
       "       [406., 428.,  56., 924.],\n",
       "       [408., 428.,  56., 924.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pixel_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([127., 126., 127., ..., 378., 404., 387.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solution_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train the model on the training data we've collected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(n_estimators=15)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def adjust_and_save(array, path: str):\n",
    "    \"\"\"Used to stretch the images contrast; making cells easier to see\"\"\"\n",
    "    lower = np.percentile(array, 0.5)\n",
    "    upper = np.percentile(array, 99.5)\n",
    "    Image.fromarray(exposure.rescale_intensity(array, in_range=(lower, upper))).save(path)\n",
    "\n",
    "model = RandomForestRegressor(n_estimators = 15)\n",
    "model.fit(pixel_data, solution_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model has been trained, we can use it to predict what the unfocused images might look like if they were optimally focused."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [1:20:50<00:00, 404.22s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>unfocused</th>\n",
       "      <th>predicted_focused</th>\n",
       "      <th>focused</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td><img src=\"./Output/_f12_s2_w2_unfocused.png\" /></td>\n",
       "      <td><img src=\"./Output/_f12_s2_w2_predict.png\" /></td>\n",
       "      <td><img src=\"./Output/_f12_s2_w2_focused.png\" /></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td><img src=\"./Output/_i18_s2_w2_unfocused.png\" /></td>\n",
       "      <td><img src=\"./Output/_i18_s2_w2_predict.png\" /></td>\n",
       "      <td><img src=\"./Output/_i18_s2_w2_focused.png\" /></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td><img src=\"./Output/_l07_s2_w2_unfocused.png\" /></td>\n",
       "      <td><img src=\"./Output/_l07_s2_w2_predict.png\" /></td>\n",
       "      <td><img src=\"./Output/_l07_s2_w2_focused.png\" /></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td><img src=\"./Output/_g21_s1_w2_unfocused.png\" /></td>\n",
       "      <td><img src=\"./Output/_g21_s1_w2_predict.png\" /></td>\n",
       "      <td><img src=\"./Output/_g21_s1_w2_focused.png\" /></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td><img src=\"./Output/_p16_s1_w2_unfocused.png\" /></td>\n",
       "      <td><img src=\"./Output/_p16_s1_w2_predict.png\" /></td>\n",
       "      <td><img src=\"./Output/_p16_s1_w2_focused.png\" /></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td><img src=\"./Output/_p07_s2_w2_unfocused.png\" /></td>\n",
       "      <td><img src=\"./Output/_p07_s2_w2_predict.png\" /></td>\n",
       "      <td><img src=\"./Output/_p07_s2_w2_focused.png\" /></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td><img src=\"./Output/_j19_s2_w2_unfocused.png\" /></td>\n",
       "      <td><img src=\"./Output/_j19_s2_w2_predict.png\" /></td>\n",
       "      <td><img src=\"./Output/_j19_s2_w2_focused.png\" /></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td><img src=\"./Output/_g08_s2_w2_unfocused.png\" /></td>\n",
       "      <td><img src=\"./Output/_g08_s2_w2_predict.png\" /></td>\n",
       "      <td><img src=\"./Output/_g08_s2_w2_focused.png\" /></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td><img src=\"./Output/_f18_s2_w1_unfocused.png\" /></td>\n",
       "      <td><img src=\"./Output/_f18_s2_w1_predict.png\" /></td>\n",
       "      <td><img src=\"./Output/_f18_s2_w1_focused.png\" /></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td><img src=\"./Output/_a24_s2_w1_unfocused.png\" /></td>\n",
       "      <td><img src=\"./Output/_a24_s2_w1_predict.png\" /></td>\n",
       "      <td><img src=\"./Output/_a24_s2_w1_focused.png\" /></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td><img src=\"./Output/_e02_s1_w1_unfocused.png\" /></td>\n",
       "      <td><img src=\"./Output/_e02_s1_w1_predict.png\" /></td>\n",
       "      <td><img src=\"./Output/_e02_s1_w1_focused.png\" /></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td><img src=\"./Output/_b12_s2_w2_unfocused.png\" /></td>\n",
       "      <td><img src=\"./Output/_b12_s2_w2_predict.png\" /></td>\n",
       "      <td><img src=\"./Output/_b12_s2_w2_focused.png\" /></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pixel_data = solution_data = None\n",
    "\n",
    "compare_df = []\n",
    "\n",
    "for pair in tqdm(test_set):\n",
    "    unfocused_file = plt.imread(pair[0])\n",
    "    output_array = unfocused_file * 0\n",
    "    \n",
    "    flat = unfocused_file.flatten()\n",
    "    mean = flat.mean() // 1\n",
    "    std = flat.std() // 1\n",
    "    ptp = np.ptp(flat)\n",
    "    \n",
    "    # For every pixel in the test set images, predict what it would look like if focused\n",
    "    for x, col in enumerate(unfocused_file):\n",
    "        for y, current_pixel in enumerate(col):\n",
    "            prediction = model.predict([[\n",
    "                current_pixel, mean, std, ptp\n",
    "            ]])\n",
    "            output_array[x][y] = prediction[0]\n",
    "\n",
    "                \n",
    "    # Converted to PNG because TIFF files don't show in browser\n",
    "    \n",
    "    pair_id = pair[0][match_start:match_end]\n",
    "    \n",
    "    unfocused_path = f'./Output/{pair_id}_unfocused.png'\n",
    "    focused_path = f'./Output/{pair_id}_focused.png'\n",
    "    output_path = f'./Output/{pair_id}_predict.png'\n",
    "    \n",
    "    adjust_and_save(plt.imread(pair[0]), unfocused_path)\n",
    "    adjust_and_save(plt.imread(pair[1]), focused_path)\n",
    "    adjust_and_save(output_array, output_path)\n",
    "    \n",
    "    compare_df.append({\n",
    "        'unfocused': f'<img src=\"{unfocused_path}\" />',\n",
    "        'predicted_focused': f'<img src=\"{output_path}\" />',\n",
    "        'focused': f'<img src=\"{focused_path}\" />'\n",
    "    })\n",
    "    \n",
    "compare_df = pd.DataFrame(compare_df)\n",
    "HTML(compare_df.to_html(escape = False, index = False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
